{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of EfficentNet in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficentNet Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Set the device to run\n",
    "# device= (\n",
    "#         \"cuda:1\"\n",
    "#         if torch.cuda.is_available()\n",
    "#         else \"mps\"\n",
    "#         if torch.backends.mps.is_available()\n",
    "#         else \"cpu\"\n",
    "#     )\n",
    "# print(f\"Using {device} device\")\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "from mycode.GenderClassificationNN import GenderClassificationNN\n",
    "from mycode.train_step import train_step\n",
    "from mycode.test_step import test_step\n",
    "from mycode.GenderDataset import GenderDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from typing import Tuple, Dict, List\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer \n",
    "import matplotlib.pyplot as plt\n",
    "import clearml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "def _RoundChannels(c, divisor=8, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_c = max(min_value, int(c + divisor / 2) // divisor * divisor)\n",
    "    if new_c < 0.9 * c:\n",
    "        new_c += divisor\n",
    "    return new_c\n",
    "\n",
    "def _RoundRepeats(r):\n",
    "    return int(math.ceil(r))\n",
    "\n",
    "def _DropPath(x, drop_prob, training):\n",
    "    if drop_prob > 0 and training:\n",
    "        keep_prob = 1 - drop_prob\n",
    "        if x.is_cuda:\n",
    "            mask = Variable(torch.cuda.FloatTensor(x.size(0), 1, 1, 1).bernoulli_(keep_prob))\n",
    "        else:\n",
    "            mask = Variable(torch.FloatTensor(x.size(0), 1, 1, 1).bernoulli_(keep_prob))\n",
    "        x.div_(keep_prob)\n",
    "        x.mul_(mask)\n",
    "\n",
    "    return x\n",
    "\n",
    "def _BatchNorm(channels, eps=1e-3, momentum=0.01):\n",
    "    return nn.BatchNorm2d(channels, eps=eps, momentum=momentum)\n",
    "\n",
    "def _Conv3x3Bn(in_channels, out_channels, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "        _BatchNorm(out_channels),\n",
    "        Swish()\n",
    "    )\n",
    "\n",
    "def _Conv1x1Bn(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n",
    "        _BatchNorm(out_channels),\n",
    "        Swish()\n",
    "    )\n",
    "\n",
    "class SqueezeAndExcite(nn.Module):\n",
    "    def __init__(self, channels, squeeze_channels, se_ratio):\n",
    "        super(SqueezeAndExcite, self).__init__()\n",
    "\n",
    "        squeeze_channels = squeeze_channels * se_ratio\n",
    "        if not squeeze_channels.is_integer():\n",
    "            raise ValueError('channels must be divisible by 1/ratio')\n",
    "\n",
    "        squeeze_channels = int(squeeze_channels)\n",
    "        self.se_reduce = nn.Conv2d(channels, squeeze_channels, 1, 1, 0, bias=True)\n",
    "        self.non_linear1 = Swish()\n",
    "        self.se_expand = nn.Conv2d(squeeze_channels, channels, 1, 1, 0, bias=True)\n",
    "        self.non_linear2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = torch.mean(x, (2, 3), keepdim=True)\n",
    "        y = self.non_linear1(self.se_reduce(y))\n",
    "        y = self.non_linear2(self.se_expand(y))\n",
    "        y = x * y\n",
    "\n",
    "        return y\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio, se_ratio, drop_path_rate):\n",
    "        super(MBConvBlock, self).__init__()\n",
    "\n",
    "        expand = (expand_ratio != 1)\n",
    "        expand_channels = in_channels * expand_ratio\n",
    "        se = (se_ratio != 0.0)\n",
    "        self.residual_connection = (stride == 1 and in_channels == out_channels)\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "\n",
    "        conv = []\n",
    "\n",
    "        if expand:\n",
    "            # expansion phase\n",
    "            pw_expansion = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, expand_channels, 1, 1, 0, bias=False),\n",
    "                _BatchNorm(expand_channels),\n",
    "                Swish()\n",
    "            )\n",
    "            conv.append(pw_expansion)\n",
    "\n",
    "        # depthwise convolution phase\n",
    "        dw = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                expand_channels,\n",
    "                expand_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                kernel_size//2,\n",
    "                groups=expand_channels,\n",
    "                bias=False\n",
    "            ),\n",
    "            _BatchNorm(expand_channels),\n",
    "            Swish()\n",
    "        )\n",
    "        conv.append(dw)\n",
    "\n",
    "        if se:\n",
    "            # squeeze and excite\n",
    "            squeeze_excite = SqueezeAndExcite(expand_channels, in_channels, se_ratio)\n",
    "            conv.append(squeeze_excite)\n",
    "\n",
    "        # projection phase\n",
    "        pw_projection = nn.Sequential(\n",
    "            nn.Conv2d(expand_channels, out_channels, 1, 1, 0, bias=False),\n",
    "            _BatchNorm(out_channels)\n",
    "        )\n",
    "        conv.append(pw_projection)\n",
    "\n",
    "        self.conv = nn.Sequential(*conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual_connection:\n",
    "            return x + _DropPath(self.conv(x), self.drop_path_rate, self.training)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    config = [\n",
    "        #(in_channels, out_channels, kernel_size, stride, expand_ratio, se_ratio, repeats)\n",
    "        [32,  16,  3, 1, 1, 0.25, 1],\n",
    "        [16,  24,  3, 2, 6, 0.25, 2],\n",
    "        [24,  40,  5, 2, 6, 0.25, 2],\n",
    "        [40,  80,  3, 2, 6, 0.25, 3],\n",
    "        [80,  112, 5, 1, 6, 0.25, 3],\n",
    "        [112, 192, 5, 2, 6, 0.25, 4],\n",
    "        [192, 320, 3, 1, 6, 0.25, 1]\n",
    "    ]\n",
    "\n",
    "    def __init__(self, param, num_classes=2, stem_channels=32, feature_size=1280, drop_connect_rate=0.2):\n",
    "        super(EfficientNet, self).__init__()\n",
    "\n",
    "        # scaling width\n",
    "        width_coefficient = param[0]\n",
    "        if width_coefficient != 1.0:\n",
    "            stem_channels = _RoundChannels(stem_channels*width_coefficient)\n",
    "            for conf in self.config:\n",
    "                conf[0] = _RoundChannels(conf[0]*width_coefficient)\n",
    "                conf[1] = _RoundChannels(conf[1]*width_coefficient)\n",
    "\n",
    "        # scaling depth\n",
    "        depth_coefficient = param[1]\n",
    "        if depth_coefficient != 1.0:\n",
    "            for conf in self.config:\n",
    "                conf[6] = _RoundRepeats(conf[6]*depth_coefficient)\n",
    "\n",
    "        # scaling resolution\n",
    "        input_size = param[2]\n",
    "\n",
    "        # stem convolution\n",
    "        self.stem_conv = _Conv3x3Bn(3, stem_channels, 2)\n",
    "\n",
    "        # total #blocks\n",
    "        total_blocks = 0\n",
    "        for conf in self.config:\n",
    "            total_blocks += conf[6]\n",
    "\n",
    "        # mobile inverted bottleneck\n",
    "        blocks = []\n",
    "        for in_channels, out_channels, kernel_size, stride, expand_ratio, se_ratio, repeats in self.config:\n",
    "            # drop connect rate based on block index\n",
    "            drop_rate = drop_connect_rate * (len(blocks) / total_blocks)\n",
    "            blocks.append(MBConvBlock(in_channels, out_channels, kernel_size, stride, expand_ratio, se_ratio, drop_rate))\n",
    "            for _ in range(repeats-1):\n",
    "                drop_rate = drop_connect_rate * (len(blocks) / total_blocks)\n",
    "                blocks.append(MBConvBlock(out_channels, out_channels, kernel_size, 1, expand_ratio, se_ratio, drop_rate))\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "        # last several layers\n",
    "        self.head_conv = _Conv1x1Bn(self.config[-1][1], feature_size)\n",
    "        #self.avgpool = nn.AvgPool2d(input_size//32, stride=1)\n",
    "        self.dropout = nn.Dropout(param[3])\n",
    "        self.classifier = nn.Linear(feature_size, num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem_conv(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.head_conv(x)\n",
    "        #x = self.avgpool(x)\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = torch.mean(x, (2, 3))\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # if __name__ == '__main__':\n",
    "    #     net_param = {\n",
    "    #         # 'efficientnet type': (width_coef, depth_coef, resolution, dropout_rate)\n",
    "    #         'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
    "    #         'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
    "    #         'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
    "    #         'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
    "    #         'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
    "    #         'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
    "    #         'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
    "    #         'efficientnet-b7': (2.0, 3.1, 600, 0.5)\n",
    "    #     }\n",
    "\n",
    "    #     param = net_param['efficientnet-b0']\n",
    "    #     net = EfficientNet(param)\n",
    "    #     x_image = Variable(torch.randn(1, 3, param[2], param[2]))\n",
    "    #     y = net(x_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset, Transforms, and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path\n",
    "train_dir = \"dataset_all/train\"\n",
    "test_dir  = \"dataset_all/test\"\n",
    "# Augment train data\n",
    "train_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        #--NEW --NOT TESTED YET\n",
    "        transforms.ColorJitter(brightness=0.5),\n",
    "        transforms.RandomRotation(45),\n",
    "        transforms.RandomVerticalFlip(p=0.05),\n",
    "        #--NEW\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    # Don't augment test data, only resize the images\n",
    "test_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "# Using custom dataset class\n",
    "train_data = GenderDataset(targ_dir=train_dir,transform=train_transforms)\n",
    "test_data = GenderDataset(targ_dir=test_dir,transform=test_transforms)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "    #Dataloader\n",
    "train_dataloader = DataLoader(dataset=train_data, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=0, \n",
    "                            shuffle=True) \n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=2, \n",
    "                            shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(results: Dict[str, List[float]]):\n",
    "    \"\"\"Plots training curves of a results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): dictionary containing list of values, e.g.\n",
    "            {\"train_loss\": [...],\n",
    "            \"train_acc\": [...],\n",
    "            \"test_loss\": [...],\n",
    "            \"test_acc\": [...]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the loss values of the results dictionary (training and test)\n",
    "    loss = results['train_loss']\n",
    "    test_loss = results['test_loss']\n",
    "\n",
    "    # Get the accuracy values of the results dictionary (training and test)\n",
    "    accuracy = results['train_acc']\n",
    "    test_accuracy = results['test_acc']\n",
    "\n",
    "    # Figure out how many epochs there were\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    # Setup a plot \n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='train_loss')\n",
    "    plt.plot(epochs, test_loss, label='test_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
    "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig('models/results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, \n",
    "        train_dataloader: torch.utils.data.DataLoader, \n",
    "        test_dataloader: torch.utils.data.DataLoader, \n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "        epochs: int = 5):\n",
    "    \n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "    \n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs), leave=True):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                        dataloader=train_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        optimizer=optimizer)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "        \n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\\n\"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_param = {\n",
    "            # 'efficientnet type': (width_coef, depth_coef, resolution, dropout_rate)\n",
    "            'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
    "            'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
    "            'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
    "            'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
    "            'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
    "            'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
    "            'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
    "            'efficientnet-b7': (2.0, 3.1, 600, 0.5)\n",
    "        }\n",
    "\n",
    "param = net_param['efficientnet-b0']\n",
    "net = EfficientNet(param)\n",
    "x_image = Variable(torch.randn(1, 3, param[2], param[2]))\n",
    "y = net(x_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model\n",
    "# model = GenderClassificationNN().to(device)\n",
    "param = net_param['efficientnet-b0']\n",
    "model = EfficientNet(param).to(device)\n",
    "# loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "#training and testing\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5460e9d24294bd794141cf305beabbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27.18GB batch 184/184: 100%|██████████| 184/184 [02:18<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.1723 | train_acc: 0.9339 | test_loss: 0.3230 | test_acc: 0.8925\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27.18GB batch 184/184: 100%|██████████| 184/184 [02:14<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.1468 | train_acc: 0.9467 | test_loss: 0.1075 | test_acc: 0.9626\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27.18GB batch 184/184: 100%|██████████| 184/184 [02:14<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 0.1301 | train_acc: 0.9520 | test_loss: 0.1010 | test_acc: 0.9649\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27.18GB batch 184/184: 100%|██████████| 184/184 [02:13<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 0.1259 | train_acc: 0.9551 | test_loss: 0.1956 | test_acc: 0.9386\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27.18GB batch 184/184: 100%|██████████| 184/184 [02:13<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 0.1191 | train_acc: 0.9567 | test_loss: 0.0874 | test_acc: 0.9687\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27.18GB batch 184/184: 100%|██████████| 184/184 [02:14<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | train_loss: 0.1154 | train_acc: 0.9595 | test_loss: 0.1296 | test_acc: 0.9556\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Set number of epochs\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "start_time = timer()\n",
    "\n",
    "model_results = train(model=model, \n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn, \n",
    "                        epochs=NUM_EPOCHS\n",
    "                    )\n",
    "end_time = timer()\n",
    "print(f\"\\nTotal training time: {end_time-start_time:.3f} seconds\")\n",
    "\n",
    "# plot the results \n",
    "plot_loss_curves(model_results)\n",
    "    \n",
    "#save the image\n",
    "torch.save(model.state_dict(), \"models/model.pth\")  #Saving models\n",
    "print(\"Saved PyTorch Model State to models/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
